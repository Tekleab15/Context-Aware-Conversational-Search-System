{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30918,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tekleab15/Context-Aware-Conversational-Search-System/blob/main/notebooks/semantic_search_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implmenting the semantic search and measuring the cosine similarity**"
      ],
      "metadata": {
        "id": "Ehmc5hPaYegc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import all required modules and mounting the drive"
      ],
      "metadata": {
        "id": "CJOkr2Av2T4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing the gradio-UI to show results\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "xX76fDwhyUcE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gradio as gr\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Run and mounted Successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T11:33:41.587569Z",
          "iopub.execute_input": "2025-03-11T11:33:41.587967Z",
          "iopub.status.idle": "2025-03-11T11:33:41.593932Z",
          "shell.execute_reply.started": "2025-03-11T11:33:41.587935Z",
          "shell.execute_reply": "2025-03-11T11:33:41.592787Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UbKJsjTYegh",
        "outputId": "0ed6ded2-5bd4-42e8-8291-7e153b5b50c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Run and mounted Successfully!\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the embeddings embeded using (SBERT)"
      ],
      "metadata": {
        "id": "A-UgbxCNCW-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-computed embeddings\n",
        "embeddings_instructions = np.load('/content/drive/My Drive/Conversational_semantic_search/embeddings_instructions.npy')\n",
        "embeddings_responses = np.load('/content/drive/My Drive/Conversational_semantic_search/embeddings_responses.npy')\n",
        "model = SentenceTransformer('all-mpnet-base-v2')\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "data_path = '/content/drive/My Drive/Conversational_semantic_search/Customer_service_preprocessed_.csv'\n",
        "data_path1 = '/content/drive/My Drive/Conversational_semantic_search/Customer_service.csv'\n",
        "df = pd.read_csv(data_path)\n",
        "df1 = pd.read_csv(data_path1)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-11T11:33:47.103846Z",
          "iopub.execute_input": "2025-03-11T11:33:47.104239Z",
          "iopub.status.idle": "2025-03-11T11:33:47.220287Z",
          "shell.execute_reply.started": "2025-03-11T11:33:47.104211Z",
          "shell.execute_reply": "2025-03-11T11:33:47.218694Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHckZRTvYegm",
        "outputId": "ee5296e4-0ddc-4e1a-834f-14e75049f4e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Search implementation without using context**"
      ],
      "metadata": {
        "id": "vZyeuNf9QDzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Response with out using context(memory of prior queries)\n",
        "def get_response_without_context(user_query):\n",
        "    global model\n",
        "    global embeddings_instructions\n",
        "    global df\n",
        "    if model is None:\n",
        "        return \"Error: Model not loaded. Try re-running the notebook.\"\n",
        "    try:\n",
        "        query_embedding = model.encode([user_query])\n",
        "        similarities = cosine_similarity(query_embedding, embeddings_instructions)\n",
        "        best_match_index = similarities.argmax()\n",
        "        best_response = df.iloc[best_match_index]['response']\n",
        "        return best_response\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "_G4J5aH21a9z"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_response_without_context(\"How can I cancel my order?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvHW3yWc1d7J",
        "outputId": "b2b86cf2-5a08-4977-8309-37d98233838f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assuredly! I understand that you want to cancel the purchase you made. Let me guide you through the process step by step: \n",
            "\n",
            "1. Log in to your {{Online Company Portal Info}} using your credentials.\n",
            "2. Navigate to the '{{Online Order Interaction}}' or '{{Online Order Interaction}}' section, where you can find a list of your previous purchases.\n",
            "3. Identify the specific purchase you want to cancel. You may need to search for it by order number or date.\n",
            "4. Once you've located the purchase, look for an option labeled '{{Online Order Interaction}}' or something similar. Click on it.\n",
            "5. Follow the instructions provided by the system to complete the cancellation process.\n",
            "\n",
            "If you encounter any difficulties or have any questions along the way, our dedicated customer support team is here to assist you. You can reach us during {{Customer Support Hours}} at {{Customer Support Phone Number}} or through the Live Chat on our website at {{Website URL}}. We value your satisfaction and will do our best to help you with your cancellation request.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Context (memory)**"
      ],
      "metadata": {
        "id": "GEnj6v36Fo--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing session memory to store the last 2-3 queries\n",
        "session_memory = []\n",
        "\n",
        "def get_response_with_context(user_query):\n",
        "    global model\n",
        "    global embeddings_instructions\n",
        "    global df\n",
        "    global session_memory\n",
        "\n",
        "    # Check if the model is loaded\n",
        "    if model is None:\n",
        "        return \"Error: Model not loaded. Please ensure the model is loaded before querying.\"\n",
        "    try:\n",
        "        # Add the current query to the session memory (limit to the last 4 queries)\n",
        "        session_memory.append(user_query)\n",
        "        # Limiting the memory size to last 4 queries\n",
        "        if len(session_memory) > 4:\n",
        "            session_memory.pop(0)\n",
        "        # Combine all previous queries to maintain context\n",
        "        context = \" \".join(session_memory)\n",
        "        context_embedding = model.encode([context])\n",
        "        # Calculate cosine similarities between the context and embeddings\n",
        "        similarities = cosine_similarity(context_embedding, embeddings_instructions)\n",
        "\n",
        "        # Identify the best matching response based on the highest similarity\n",
        "        best_match_index = similarities.argmax()\n",
        "        best_response = df.iloc[best_match_index]['response']\n",
        "        return best_response\n",
        "    except Exception as e:\n",
        "        # Handle any unexpected errors\n",
        "        return f\"An error occurred: {str(e)}\"\n"
      ],
      "metadata": {
        "id": "CMEUf0QQFn3v"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding Intent Recongnition**"
      ],
      "metadata": {
        "id": "b3Ew5RL-oMCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candidate_labels = list(df1['intent'].unique())\n",
        "candidate_embeddings = model.encode(candidate_labels, convert_to_tensor=True)\n",
        "def predict_intent(user_query):\n",
        "  try:\n",
        "    query_embedding = model.encode([user_query], convert_to_tensor=True)\n",
        "    cos_scores = util.cos_sim(query_embedding, candidate_embeddings)\n",
        "    best_idx = cos_scores.argmax().item()\n",
        "    return candidate_labels[best_idx]\n",
        "  except Exception as e:\n",
        "    return f\"An error occurred: {str(e)}\""
      ],
      "metadata": {
        "id": "Ruwb4zv6pOcO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def chat_with_intent(user_query, response_mode):\n",
        "def chat_with_intent(user_query, response_mode):\n",
        "    print(\"Response Mode Selected:\", response_mode)  # Debug print\n",
        "    intent = predict_intent(user_query)\n",
        "    if response_mode == \"With Context\":\n",
        "        response = get_response_with_context(user_query)\n",
        "    else:\n",
        "        response = get_response_without_context(user_query)\n",
        "    return f\"Intent: {intent}\\nResponse: {response}\"\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chat_with_intent,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Your Query\", placeholder=\"Enter text here...\"),\n",
        "        gr.Radio([\"Without Context\", \"With Context\"], label=\"Response Mode\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Intent Detection with SBERT\",\n",
        "    description=\"Classifies a user query's intent using SBERT embeddings and cosine similarity.\"\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "befXh3Rwq8_H",
        "outputId": "efd1bc37-00ef-4a06-bd14-df04ff716ce6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d7ee429f735e22c4b3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d7ee429f735e22c4b3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}